{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mwn-fqCZiQYw"
   },
   "source": [
    "### GeoPython 2019: Workshop Deep Learning using Airborne Imagery\n",
    "# Example 1: Classification of Land Use with a Simple Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2u_41dyHtwjG"
   },
   "source": [
    "## Workspace setup\n",
    "The following steps refer to setting up the workspace in Google CoLab. If you want to run the notebook locally, you can clone the GitHub repository containing the notebook and all data. Please note, that you may need to change some paths according to where you stored the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKDMcuRoptzq"
   },
   "source": [
    "### Download the data\n",
    "Let's first check if the data is already downloaded, and when not do so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mFBYFrI8Bbxu"
   },
   "outputs": [],
   "source": [
    "!if ! [ -d /geopython/ ]; then mkdir /geopython; curl -s -L -o /geopython/data.zip https://www.dropbox.com/s/3aogabch1d2sdu8/data.zip?dl=0; unzip -q /geopython/data.zip -d /geopython/; rm /geopython/data.zip; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k_yUCUoykdRJ"
   },
   "source": [
    "### Install additional module\n",
    "Then we need to install the missing package pandas_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n3G5cg3kjxig"
   },
   "outputs": [],
   "source": [
    "!pip install pandas_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2L8I5LCtkske"
   },
   "source": [
    "### Import of all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rx-VF2f_NM0-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists, isdir, join, splitext\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_ml import ConfusionMatrix\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "\n",
    "pd.set_option('display.max_columns', 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgdnWLMv9PNT"
   },
   "source": [
    "### Definition of global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QEzDOZfoiu89"
   },
   "outputs": [],
   "source": [
    "TILE_SIZE = 100\n",
    "BATCH_SIZE = 20\n",
    "ROOT_PATH = '/geopython/'\n",
    "DATA_PATH = join(ROOT_PATH, 'data')\n",
    "PRE_PATH = join(ROOT_PATH, 'models/pretrained.h5')\n",
    "MODEL_PATH = join(ROOT_PATH, 'models/areal.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sa0AgzJwlE_y"
   },
   "source": [
    "## Definition of model architecture\n",
    "In this example, we will implement a very simple Convolutional Neural Network (CNN). The code is based on the book \"Deep Learning with Python\" by François Chollet, but adapted to handle aerial imagery instead of cats and dogs.\n",
    "\n",
    "![Example Architecture](imgs/convnet_architecture.png)\n",
    "\n",
    "Our network consists of four Convolutional layers, each followed by a MaxPooling layer, and two Fully Connected layers at the end. This simple architecture is chosen, to show the fundamental principles of deep learning with CNNs. For productive use, this network could be exchanged with a much deeper architecture, like ResNet or GoogLeNet. These networks will get you better results, but their training takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvmIKXUUiVtg"
   },
   "outputs": [],
   "source": [
    "class ConvNet(keras.models.Sequential):\n",
    "    \"\"\"\n",
    "    Definition of a simple CNN architecture based on Keras sequential model\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, num_classes, dropout=0.5):\n",
    "        \"\"\"\n",
    "\n",
    "        :param img_size: Image size of tiles in pixel\n",
    "        :param num_classes: Number of classes to distinguish\n",
    "        :param dropout: Optional definition of dropout rate\n",
    "                        If 'None' is passed, there will be no dropout layer\n",
    "        \"\"\"\n",
    "        super().__init__()  # Initialize sequential model\n",
    "\n",
    "        # Definition of the network's layer structure\n",
    "        # For Conv-layers, the number and size of filter kernels is defined\n",
    "        self.add(keras.layers.Conv2D(32, (3, 3), activation='relu', \n",
    "                                     input_shape=(img_size, img_size, 3)))\n",
    "        # For pooling-layers, the number of pixels to be pooled is defined\n",
    "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        self.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        self.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        self.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "        # Flattening of the resulting feature map to get a 1D-vector\n",
    "        self.add(keras.layers.Flatten())\n",
    "\n",
    "        # Add dropout layer, if defined\n",
    "        if dropout:\n",
    "            self.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "        # Fully-Connected layers for classification\n",
    "        self.add(keras.layers.Dense(512, activation='relu'))\n",
    "        self.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hI_DiPSmlNXk"
   },
   "source": [
    "## Definition of dataset classes\n",
    "\n",
    "This base class helps us loading the correct data for each class during training, validation and testing. It actually just stores the paths of the respective directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULf9Sh2IihDU"
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Definition of dataset base class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory, classes, names=None):\n",
    "        \"\"\"\n",
    "        Initialize a directory as dataset\n",
    "\n",
    "        :param directory: Root directory as string\n",
    "        :param classes: List or tuple containing the names of the subdirectories as strings\n",
    "        :param names: Optional class names (If not defined, directory names will be used)\n",
    "        \"\"\"\n",
    "        self.base_dir = directory\n",
    "        self.classes = classes\n",
    "        self.cls_names = names or classes\n",
    "\n",
    "    @property\n",
    "    def train_dir(self):\n",
    "        return join(self.base_dir, 'train')\n",
    "\n",
    "    @property\n",
    "    def validation_dir(self):\n",
    "        return join(self.base_dir, 'val')\n",
    "\n",
    "    @property\n",
    "    def test_dir(self):\n",
    "        return join(self.base_dir, 'test')\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Prints a summary of the dataset\n",
    "        \"\"\"\n",
    "        row_format = '{:>' + str(max(len(max(self.cls_names, key=lambda x: len(x))), 6) + 1) + '}' + '{:>12}' * 3 + '\\n'\n",
    "        text = row_format.format('Class', 'Train', 'Val', 'Test')\n",
    "        text += '-' * len(text) + '\\n'\n",
    "        for cls, name in zip(self.classes, self.cls_names):\n",
    "            text += row_format.format(name,\n",
    "                                      len(listdir(join(self.train_dir, cls))),\n",
    "                                      len(listdir(join(self.validation_dir, cls))),\n",
    "                                      len(listdir(join(self.test_dir, cls))))\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8BJTGOYuj_W"
   },
   "source": [
    "### Swiss Land Use Dataset (\"Arealstatistik\")\n",
    "\n",
    "The swiss land use dataset consists of a grid with a mesh size of 100 meters. Each grid point gets assigned to one of 72 different classes. In this example we will only use the four classes 'Urban', 'Rural', 'Forest' and 'Other', which are actually just aggregated from the detailed labels.\n",
    "\n",
    "![Example Images](imgs/dataset_examples.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCcl0Q-NlUh4"
   },
   "outputs": [],
   "source": [
    "class ArealStat4(Dataset):\n",
    "    \"\"\"\n",
    "    Predefined class for land use dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, directory):\n",
    "        super().__init__(directory, ('1', '2', '3', '4'), ('Urban', 'Rural', 'Forest', 'Other'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMeWPn_E_aTV"
   },
   "source": [
    "## Instantiation of dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOY-NE17418G"
   },
   "outputs": [],
   "source": [
    "data = ArealStat4(DATA_PATH)\n",
    "model = ConvNet(TILE_SIZE, len(data.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiW71hjzlX7H"
   },
   "source": [
    "## Training of the network\n",
    "To train the network, we first have to choose a loss function and an optimizer.\n",
    "\n",
    "Then we create data generators that prepare the images for feeding them into the network. These generators load the images from the directories specified in the dataset class.\n",
    "\n",
    "With the ```fit_generator``` method, we actually start the training process. This is where we can choose the duration of the training (```epochs```).\n",
    "\n",
    "After training, it's always recommended to save the model for using it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jK2EuvV2i26v"
   },
   "outputs": [],
   "source": [
    "# Definition of loss function and optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Definition of generators for training and validation datasets\n",
    "train_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "train_data = train_gen.flow_from_directory(data.train_dir,\n",
    "                                           target_size=(TILE_SIZE, TILE_SIZE),\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           class_mode='categorical')\n",
    "\n",
    "val_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "val_data = val_gen.flow_from_directory(data.validation_dir,\n",
    "                                       target_size=(TILE_SIZE, TILE_SIZE),\n",
    "                                       batch_size=BATCH_SIZE,\n",
    "                                       class_mode='categorical')\n",
    "\n",
    "# Run training and save history values for plotting later\n",
    "history = model.fit_generator(train_data,\n",
    "                              steps_per_epoch=150,\n",
    "                              epochs=20,\n",
    "                              validation_data=val_data,\n",
    "                              validation_steps=25)\n",
    "\n",
    "# Create snapshot of model weights\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kJqC3SiEA1QB"
   },
   "source": [
    "### Create plots\n",
    "\n",
    "These line plots give us a sneak peak on how the network performed during the training process. Ideally, the accuracy values are getting bigger and the loss is striving to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3kqCF5cU6zKX"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "\n",
    "# Create plot for accuracy values\n",
    "plt.plot(epochs, history.history['acc'], 'r', label='Training acc')\n",
    "plt.plot(epochs, history.history['val_acc'], 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Create plot for loss values\n",
    "plt.figure()\n",
    "plt.plot(epochs, history.history['loss'], 'r', label='Training loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-Di849FldZ0"
   },
   "source": [
    "## Evaluation of trained model\n",
    "\n",
    "To evaulate, how good the model has learned to classify land use, we predict the classes of the test dataset. These are images, the network has never seen during the training process.\n",
    "\n",
    "By comparing the predicted labels with the ground truth, we can calculate key values as precision (which percentage of images classified as 'urban' actually show 'urban') and recall (which percentage of all images showing 'rural' were correctly classified) for each class and for the model in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgZmq_TTi4EV"
   },
   "outputs": [],
   "source": [
    "# Load correct model weights\n",
    "model.load_weights(MODEL_PATH)\n",
    "\n",
    "ground_truth = []\n",
    "prediction = []\n",
    "\n",
    "# Iterate over all images of test dataset\n",
    "for i, c in enumerate(data.classes):\n",
    "    for img in [f for f in os.listdir(join(data.test_dir,c)) if f.split('.')[-1] == 'jpg']:\n",
    "            # Load image and convert to 32-bit float in range [0,1]\n",
    "            raw = Image.open(join(data.test_dir, c, img))\n",
    "            raw = raw.resize((TILE_SIZE, TILE_SIZE))\n",
    "            rgb = raw.convert(\"RGB\")\n",
    "            arr = np.asarray(rgb, dtype='float32') / 255\n",
    "            arr = np.expand_dims(arr, axis=0)\n",
    "\n",
    "            # Feed through network\n",
    "            out = model.predict(arr, batch_size=1)\n",
    "\n",
    "            ground_truth.append(i)\n",
    "            prediction.append(np.argmax(out[0]))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = ConfusionMatrix(ground_truth, prediction)\n",
    "cm.print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RVu_-Dlpljfj"
   },
   "source": [
    "## Inference with new data\n",
    "\n",
    "When the accuracy of trained model is good enough, we can use it to classify any kind of data. To show this, we load an aerial image with arbitrary size. Then we create tiles with the same size as the patches in our training dataset, which we can feed through our trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNHsDKq1ql9-"
   },
   "source": [
    "### Load image and classify its tiles\n",
    "We load an image and the pretrained network weights. If you succeeded in training your own weights, you can change it to ```MODEL_PATH``` for using them.\n",
    "\n",
    "Then we determine into how many tiles our image can be divided and store the first corner for each tile in a list. With this list, we can get the pixel values for each tile and pass them to the network for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6bkGKYU-lx4r"
   },
   "outputs": [],
   "source": [
    "# Definition of file path\n",
    "OP_PATH = join(ROOT_PATH, 'orthoimage.tiff')\n",
    "\n",
    "# Load correct model weights\n",
    "model.load_weights(PRE_PATH)\n",
    "\n",
    "# Load image\n",
    "img = Image.open(OP_PATH)\n",
    "width, height = img.size\n",
    "vals = np.array(img)\n",
    "\n",
    "tiles = []\n",
    "classes = []\n",
    "\n",
    "# Partition image into tiles with same size as training examples\n",
    "for i in range(0, width, TILE_SIZE):\n",
    "    for j in range(0, height, TILE_SIZE):\n",
    "        if i+TILE_SIZE > width or j+TILE_SIZE > height:\n",
    "            continue\n",
    "        tiles.append((i, j))\n",
    "\n",
    "# Batch-wise classify all tiles\n",
    "for b in range(0, len(tiles), BATCH_SIZE):\n",
    "    input_data = np.array([vals[i:i+TILE_SIZE, j:j+TILE_SIZE] for j, i in tiles[b:b+BATCH_SIZE]])\n",
    "    labels = model.predict(input_data, batch_size=BATCH_SIZE)\n",
    "    classes += [np.argmax(i) for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebsy08lGDee0"
   },
   "source": [
    "### Export results to csv file\n",
    "When classifying a aerial image, we actually want to connect our results to coordinate values. Therefore we parse world file corresponding to our image. This text file contains the transformation parameters for converting pixel coordinates to a national or global coordinate system. Then we can write a csv file, which contatins the classification resuts and the respective coordinates.\n",
    "\n",
    "#### Worldfile basics\n",
    "A world file is just a simple text file, containing six parameters for the transformation, each on a separate line.\n",
    "* x-component of pixel width\n",
    "* y-component of pixel width\n",
    "* x-component of pixel height\n",
    "* y-component of pixel height\n",
    "* x-coordinate of upper left pixel (center of pixel)\n",
    "* y-coordinate of upper left pixel (center of pixel)\n",
    "\n",
    "In the easiest case, the fourth line is the negative value of the first line and the second and third lines are zero. In this case we have square pixels that are parallel to the target coordinate system.\n",
    "\n",
    "#### Default values\n",
    "If there is no world file for an image, we set some default values for the coordinate export. The origin of the image is set to 0/0 and the pixel width and height to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BH5OJkMkwrsx"
   },
   "outputs": [],
   "source": [
    "DEFAULT_GEO_REF = ((0, 0), (1, 0), (0, -1))\n",
    "WORLD_FILE_EXTENSIONS = {'.tif': 'tfw', '.tiff': 'tfw', '.png': 'pgw', '.jpg': 'jgw', '.jpeg': 'jgw', '.gif': 'gfw'}\n",
    "\n",
    "\n",
    "def create_classification_export(tiles, classes, image_path, tile_size=TILE_SIZE):\n",
    "    \"\"\"\n",
    "    Export the classification results to a csv file\n",
    "    :param tiles: List containing the pixel coordinates of all tile corners\n",
    "    :param classes: List containing the classification results\n",
    "    :param image_path: Path to image file\n",
    "    :param tile_size: Tile size in pixels\n",
    "    \"\"\"\n",
    "    # Determine path to world file from image path\n",
    "    world_file = '{}.{}'.format(splitext(image_path)[0],\n",
    "                                WORLD_FILE_EXTENSIONS.get(splitext(image_path)[1], 'undefined'))\n",
    "\n",
    "    with open('{}_cls.csv'.format(splitext(image_path)[0]), 'w') as fid:\n",
    "        tiles_world = calculate_tile_centers(tiles, world_file, tile_size)\n",
    "\n",
    "        for position, cls in zip(tiles_world, classes):\n",
    "            fid.write('{},{},{}\\n'.format(*position, cls))\n",
    "\n",
    "\n",
    "def calculate_tile_centers(tiles_img, world_file, tile_size=TILE_SIZE):\n",
    "    \"\"\"\n",
    "    Calculate center coordinates for all tiles contained in image\n",
    "    :param tiles_img: List containing the pixel coordinates of all tile corners\n",
    "    :param world_file: Path to world file\n",
    "    :param tile_size: Tile size in pixels\n",
    "    :return: List containing the coordinates of all tile center points\n",
    "    \"\"\"\n",
    "    if exists(world_file):\n",
    "        georef = parse_worldfile(world_file)\n",
    "    else:\n",
    "        georef = DEFAULT_GEO_REF\n",
    "\n",
    "    return [(georef[0][0] + georef[1][0]*(x+tile_size/2) + georef[1][1]*(y+tile_size/2),\n",
    "            georef[0][1] + georef[2][0]*(x+tile_size/2) + georef[2][1]*(y+tile_size/2))\n",
    "            for x, y in tiles_img]\n",
    "\n",
    "\n",
    "def parse_worldfile(path):\n",
    "    \"\"\"\n",
    "    Parse a world file\n",
    "    :param path: Path to world file as string\n",
    "    :return: Tuple containing the georeferencing values\n",
    "    \"\"\"\n",
    "    with open(path) as fid:\n",
    "        values = [float(v.strip()) for v in fid.readlines()]\n",
    "\n",
    "    return ((values[4] - 0.5*values[0] - 0.5*values[2],\n",
    "            values[5] - 0.5*values[1] - 0.5*values[3]),\n",
    "            tuple(values[0:2]), tuple(values[2:4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actually write csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWoR_tPjqHZE"
   },
   "outputs": [],
   "source": [
    "create_classification_export(tiles, [data.cls_names[c] for c in classes], OP_PATH, TILE_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e8l5k-2sqVVL"
   },
   "source": [
    "### Create overlay image for visualization\n",
    "\n",
    "For visually analyzing our results, we create an overlay for the aerial image, where every tile gets coloured depending on its classification result. This grid is then alpha-blended onto the actual image so we can see the image and the results combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxgOhllJqK0y"
   },
   "outputs": [],
   "source": [
    "COLORS = ['red', 'yellow', 'green', 'blue']\n",
    "\n",
    "ovr = Image.new('RGB', img.size, 'white')\n",
    "for anchor, cls in zip(tiles, classes):\n",
    "    ovr.paste(Image.new('RGB', (TILE_SIZE, TILE_SIZE), COLORS[cls]), anchor)\n",
    "out = Image.blend(img, ovr, 0.4)\n",
    "out.save('_cls'.join(splitext(OP_PATH)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zr7-9dcUlsIX"
   },
   "source": [
    "## Further ressources\n",
    "* [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python): Beginner-friendly introduction to Deep Learning by François Chollet\n",
    "* [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning): Curated list containing great ressources about Deep Learning"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LandUseClassification.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
