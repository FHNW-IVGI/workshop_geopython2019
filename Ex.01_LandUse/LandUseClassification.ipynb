{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LandUseClassification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwn-fqCZiQYw",
        "colab_type": "text"
      },
      "source": [
        "### GeoPython 2019: Workshop Deep Learning using Airborne Imagery\n",
        "# Example 1: Classification of Land Use with a Simple Convolutional Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u_41dyHtwjG",
        "colab_type": "text"
      },
      "source": [
        "## Workspace setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKDMcuRoptzq",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "Let's first check if the data is already downloaded, and when not do so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFBYFrI8Bbxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!if ! [ -d /content/data ]; then curl -s -L -o /content/data.zip https://www.dropbox.com/s/tlaqa428b4m9kyv/data.zip?dl=0; unzip -q /content/data.zip -d /content/; rm /content/data.zip; fi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yUCUoykdRJ",
        "colab_type": "text"
      },
      "source": [
        "### Install additional module\n",
        "Then we need to install the missing package pandas_ml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3G5cg3kjxig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pandas_ml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L8I5LCtkske",
        "colab_type": "text"
      },
      "source": [
        "### Import of all modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx-VF2f_NM0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from os.path import exists, isdir, join, splitext\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas_ml import ConfusionMatrix\n",
        "from PIL import Image\n",
        "from tensorflow import keras\n",
        "\n",
        "pd.set_option('display.max_columns', 25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgdnWLMv9PNT",
        "colab_type": "text"
      },
      "source": [
        "### Definition of global constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEzDOZfoiu89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TILE_SIZE = 100\n",
        "BATCH_SIZE = 20\n",
        "MODEL_PATH = '/content/data/areal.h5'\n",
        "PRE_PATH = '/content/data/areal_.h5'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa0AgzJwlE_y",
        "colab_type": "text"
      },
      "source": [
        "## Definition of model architecture\n",
        "\n",
        "![Example Architecture](imgs/convnet_architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvmIKXUUiVtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(keras.models.Sequential):\n",
        "    \"\"\"\n",
        "    Definition of a simple CNN architecture based on Keras sequential model\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size, num_classes, dropout=0.5):\n",
        "        \"\"\"\n",
        "\n",
        "        :param img_size: Image size of tiles in pixel\n",
        "        :param num_classes: Number of classes to distinguish\n",
        "        :param dropout: Optional definition of dropout rate\n",
        "                        If 'None' is passed, there will be no dropout layer\n",
        "        \"\"\"\n",
        "        super().__init__()  # Initialize sequential model\n",
        "\n",
        "        # Definition of the network's layer structure\n",
        "        # For Conv-layers, the number and size of filter kernels is defined\n",
        "        self.add(keras.layers.Conv2D(32, (3, 3), activation='relu', \n",
        "                                     input_shape=(img_size, img_size, 3)))\n",
        "        # For pooling-layers, the number of pixels to be pooled is defined\n",
        "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "        self.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "        self.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "        self.add(keras.layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "        self.add(keras.layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "        # Flattening of the resulting feature map to get a 1D-vector\n",
        "        self.add(keras.layers.Flatten())\n",
        "\n",
        "        # Add dropout layer, if defined\n",
        "        if dropout:\n",
        "            self.add(keras.layers.Dropout(dropout))\n",
        "\n",
        "        # Fully-Connected layers for classification\n",
        "        self.add(keras.layers.Dense(512, activation='relu'))\n",
        "        self.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI_DiPSmlNXk",
        "colab_type": "text"
      },
      "source": [
        "## Definition of dataset classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULf9Sh2IihDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(object):\n",
        "    \"\"\"\n",
        "    Definition of dataset base class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, directory, classes, names=None):\n",
        "        \"\"\"\n",
        "        Initialize a directory as dataset\n",
        "\n",
        "        :param directory: Root directory as string\n",
        "        :param classes: List or tuple containing the names of the subdirectories as strings\n",
        "        :param names: Optional class names (If not defined, directory names will be used)\n",
        "        \"\"\"\n",
        "        self.base_dir = directory\n",
        "        self.classes = classes\n",
        "        self.cls_names = names or classes\n",
        "\n",
        "    @property\n",
        "    def train_dir(self):\n",
        "        return join(self.base_dir, 'train')\n",
        "\n",
        "    @property\n",
        "    def validation_dir(self):\n",
        "        return join(self.base_dir, 'val')\n",
        "\n",
        "    @property\n",
        "    def test_dir(self):\n",
        "        return join(self.base_dir, 'test')\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Prints a summary of the dataset\n",
        "        \"\"\"\n",
        "        row_format = '{:>' + str(max(len(max(self.cls_names, key=lambda x: len(x))), 6) + 1) + '}' + '{:>12}' * 3 + '\\n'\n",
        "        text = row_format.format('Class', 'Train', 'Val', 'Test')\n",
        "        text += '-' * len(text) + '\\n'\n",
        "        for cls, name in zip(self.classes, self.cls_names):\n",
        "            text += row_format.format(name,\n",
        "                                      len(listdir(join(self.train_dir, cls))),\n",
        "                                      len(listdir(join(self.validation_dir, cls))),\n",
        "                                      len(listdir(join(self.test_dir, cls))))\n",
        "        print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8BJTGOYuj_W",
        "colab_type": "text"
      },
      "source": [
        "### Swiss Land Use Dataset (\"Arealstatistik\")\n",
        "![Example Urban](imgs/ex_urban.jpg) ![Example Rural](imgs/ex_rural.jpg) ![Example Forest](imgs/ex_forest.jpg) ![Example Other](imgs/ex_other.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCcl0Q-NlUh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ArealStat4(Dataset):\n",
        "    \"\"\"\n",
        "    Predefined class for land use dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, directory):\n",
        "        super().__init__(directory, ('1', '2', '3', '4'), ('Urban', 'Rural', 'Forest', 'Other'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMeWPn_E_aTV",
        "colab_type": "text"
      },
      "source": [
        "## Instantiation of dataset and model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOY-NE17418G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = ArealStat4('/content/data')\n",
        "model = ConvNet(TILE_SIZE, len(data.classes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiW71hjzlX7H",
        "colab_type": "text"
      },
      "source": [
        "## Training of the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jK2EuvV2i26v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of loss function and optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=keras.optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# Definition of generators for training and validation datasets\n",
        "train_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
        "train_data = train_gen.flow_from_directory(data.train_dir,\n",
        "                                           target_size=(TILE_SIZE, TILE_SIZE),\n",
        "                                           batch_size=BATCH_SIZE,\n",
        "                                           class_mode='categorical')\n",
        "\n",
        "val_gen = keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
        "val_data = val_gen.flow_from_directory(data.validation_dir,\n",
        "                                       target_size=(TILE_SIZE, TILE_SIZE),\n",
        "                                       batch_size=BATCH_SIZE,\n",
        "                                       class_mode='categorical')\n",
        "\n",
        "# Run training and save history values for plotting later\n",
        "history = model.fit_generator(train_data,\n",
        "                              steps_per_epoch=150,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_data,\n",
        "                              validation_steps=25)\n",
        "\n",
        "# Create snapshot of model weights\n",
        "model.save(MODEL_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJqC3SiEA1QB",
        "colab_type": "text"
      },
      "source": [
        "### Create plots of values during training process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kqCF5cU6zKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "\n",
        "# Create plot for accuracy values\n",
        "plt.plot(epochs, history.history['acc'], 'r', label='Training acc')\n",
        "plt.plot(epochs, history.history['val_acc'], 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Create plot for loss values\n",
        "plt.figure()\n",
        "plt.plot(epochs, history.history['loss'], 'r', label='Training loss')\n",
        "plt.plot(epochs, history.history['val_loss'], 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Di849FldZ0",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation of trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgZmq_TTi4EV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load correct model weights\n",
        "model.load_weights(MODEL_PATH)\n",
        "\n",
        "ground_truth = []\n",
        "prediction = []\n",
        "\n",
        "# Iterate over all images of test dataset\n",
        "for i, c in enumerate(data.classes):\n",
        "    for img in [f for f in os.listdir(join(data.test_dir,c)) if f.split('.')[-1] == 'jpg']:\n",
        "            # Load image and convert to 32-bit float in range [0,1]\n",
        "            raw = Image.open(join(data.test_dir, c, img))\n",
        "            raw = raw.resize((TILE_SIZE, TILE_SIZE))\n",
        "            rgb = raw.convert(\"RGB\")\n",
        "            arr = np.asarray(rgb, dtype='float32') / 255\n",
        "            arr = np.expand_dims(arr, axis=0)\n",
        "\n",
        "            # Feed through network\n",
        "            out = model.predict(arr, batch_size=1)\n",
        "\n",
        "            ground_truth.append(i)\n",
        "            prediction.append(np.argmax(out[0]))\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = ConfusionMatrix(ground_truth, prediction)\n",
        "cm.print_stats()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVu_-Dlpljfj",
        "colab_type": "text"
      },
      "source": [
        "## Inference with new data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebsy08lGDee0",
        "colab_type": "text"
      },
      "source": [
        "### Define some utility functions for handling world files\n",
        "\n",
        "#### Worldfile basics\n",
        "\n",
        "World-File enthält die x- und y-Komponenten pro Pixel und die Zentrumskoordinaten des Pixels oben links für die Koordinaten der Bildecke muss ein halbes Pixel subtrahiert werden\n",
        "\n",
        "#### Default values\n",
        "Falls kein World-File existiert, werden Default-Werte gesetzt: Die obere linke Bildecke erhält die Koordinaten 0/0 und es wird angenommen, dass die Pixel parallel zum Koordinatensystem liegen. Als Default-Wert für die Bodenauflösung wird 1 Meter verwendet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH5OJkMkwrsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_GEO_REF = ((0, 0), (1, 0), (0, -1))\n",
        "WORLD_FILE_EXTENSIONS = {'.tif': 'tfw', '.tiff': 'tfw', '.png': 'pgw', '.jpg': 'jgw', '.jpeg': 'jgw', '.gif': 'gfw'}\n",
        "\n",
        "\n",
        "def create_classification_export(tiles, classes, image_path, tile_size=TILE_SIZE):\n",
        "    \"\"\"\n",
        "    Export the classification results to a csv file\n",
        "    :param tiles: List containing the pixel coordinates of all tile corners\n",
        "    :param classes: List containing the classification results\n",
        "    :param image_path: Path to image file\n",
        "    :param tile_size: Tile size in pixels\n",
        "    \"\"\"\n",
        "    # Determine path to world file from image path\n",
        "    world_file = '{}.{}'.format(splitext(image_path)[0],\n",
        "                                WORLD_FILE_EXTENSIONS.get(splitext(image_path)[1], 'undefined'))\n",
        "\n",
        "    with open('{}_cls.csv'.format(splitext(image_path)[0]), 'w') as fid:\n",
        "        tiles_world = calculate_tile_centers(tiles, world_file, tile_size)\n",
        "\n",
        "        for position, cls in zip(tiles_world, classes):\n",
        "            fid.write('{},{},{}\\n'.format(*position, cls))\n",
        "\n",
        "\n",
        "def calculate_tile_centers(tiles_img, world_file, tile_size=TILE_SIZE):\n",
        "    \"\"\"\n",
        "    Calculate center coordinates for all tiles contained in image\n",
        "    :param tiles_img: List containing the pixel coordinates of all tile corners\n",
        "    :param world_file: Path to world file\n",
        "    :param tile_size: Tile size in pixels\n",
        "    :return: List containing the coordinates of all tile center points\n",
        "    \"\"\"\n",
        "    if exists(world_file):\n",
        "        georef = parse_worldfile(world_file)\n",
        "    else:\n",
        "        georef = DEFAULT_GEO_REF\n",
        "\n",
        "    return [(georef[0][0] + georef[1][0]*(x+tile_size/2) + georef[1][1]*(y+tile_size/2),\n",
        "            georef[0][1] + georef[2][0]*(x+tile_size/2) + georef[2][1]*(y+tile_size/2))\n",
        "            for x, y in tiles_img]\n",
        "\n",
        "\n",
        "def parse_worldfile(path):\n",
        "    \"\"\"\n",
        "    Parse a world file\n",
        "    :param path: Path to world file as string\n",
        "    :return: Tuple containing the georeferencing values\n",
        "    \"\"\"\n",
        "    with open(path) as fid:\n",
        "        values = [float(v.strip()) for v in fid.readlines()]\n",
        "\n",
        "    return ((values[4] - 0.5*values[0] - 0.5*values[2],\n",
        "            values[5] - 0.5*values[1] - 0.5*values[3]),\n",
        "            tuple(values[0:2]), tuple(values[2:4]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNHsDKq1ql9-",
        "colab_type": "text"
      },
      "source": [
        "### Load image and classify its tiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bkGKYU-lx4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definition of file path\n",
        "OP_PATH = '/Orthophoto.tiff'\n",
        "\n",
        "# Load correct model weights\n",
        "model.load_weights(PRE_PATH)\n",
        "\n",
        "# Load image\n",
        "img = Image.open(OP_PATH)\n",
        "width, height = img.size\n",
        "vals = np.array(img)\n",
        "\n",
        "tiles = []\n",
        "classes = []\n",
        "\n",
        "# Partition image into tiles with same size as training examples\n",
        "for i in range(0, width, TILE_SIZE):\n",
        "    for j in range(0, height, TILE_SIZE):\n",
        "        if i+TILE_SIZE > width or j+TILE_SIZE > height:\n",
        "            continue\n",
        "        tiles.append((i, j))\n",
        "\n",
        "# Batch-wise classify all tiles\n",
        "for b in range(0, len(tiles), BATCH_SIZE):\n",
        "    input_data = np.array([vals[i:i+TILE_SIZE, j:j+TILE_SIZE] for j, i in tiles[b:b+BATCH_SIZE]])\n",
        "    labels = model.predict(input_data, batch_size=BATCH_SIZE)\n",
        "    classes += [np.argmax(i) for i in labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-dLrXSxqRdr",
        "colab_type": "text"
      },
      "source": [
        "### Export results to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWoR_tPjqHZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_classification_export(tiles, [data.cls_names[c] for c in classes], OP_PATH, TILE_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8l5k-2sqVVL",
        "colab_type": "text"
      },
      "source": [
        "### Create overlay image for visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxgOhllJqK0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "COLORS = ['red', 'yellow', 'green', 'blue']\n",
        "\n",
        "ovr = Image.new('RGB', img.size, 'white')\n",
        "for anchor, cls in zip(tiles, classes):\n",
        "    ovr.paste(Image.new('RGB', (TILE_SIZE, TILE_SIZE), COLORS[cls]), anchor)\n",
        "out = Image.blend(img, ovr, 0.4)\n",
        "out.save('_cls'.join(splitext(OP_PATH)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr7-9dcUlsIX",
        "colab_type": "text"
      },
      "source": [
        "## Further ressources\n",
        "* [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python): Beginner-friendly introduction to Deep Learning by François Chollet\n",
        "* [Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning): Curated list containing great ressources about Deep Learning"
      ]
    }
  ]
}